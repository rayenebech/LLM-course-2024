{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-Context Learning\n",
    "\n",
    "\n",
    "In-context learning is a generalisation of few-shot learning where the LLM is provided a context as part of the prompt and asked to respond by utilising the information in the context.\n",
    "\n",
    "* Example: *\"Summarize this research article into one paragraph highlighting its strengths and weaknesses: [insert article text]”*\n",
    "* Example: *\"Extract all the quotes from this text and organize them in alphabetical order: [insert text]”*\n",
    "\n",
    "A very popular technique that you will learn in week 5 called Retrieval-Augmented Generation (RAG) is a form of in-context learning, where:\n",
    "* a search engine is used to retrieve some relevant information\n",
    "* that information is then provided to the LLM as context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we download some recent research papers from arXiv papers, extract the text from the PDF files and ask Gemini to summarize the articles as well as provide the main strengths and weaknesses of the papers. Finally we print the summaries to a local html file and as markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import google.generativeai as genai\n",
    "from urllib.request import urlopen, urlretrieve\n",
    "from IPython.display import Markdown, display\n",
    "from pypdf import PdfReader\n",
    "from datetime import date\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = os.environ.get(\"GEMINI_API_KEY\")\n",
    "genai.configure(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select those papers that have been featured in Hugging Face papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://huggingface.co/papers\"\n",
    "page = requests.get(BASE_URL)\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "h3s = soup.find_all(\"h3\")\n",
    "\n",
    "papers = []\n",
    "\n",
    "for h3 in h3s:\n",
    "    a = h3.find(\"a\")\n",
    "    title = a.text\n",
    "    link = a[\"href\"].replace('/papers', '')\n",
    "\n",
    "    papers.append({\"title\": title, \"url\": f\"https://arxiv.org/pdf{link}\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to extract text from PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_paper(url):\n",
    "    html = urlopen(url).read()\n",
    "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "\n",
    "    # kill all script and style elements\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()    # rip it out\n",
    "\n",
    "    # get text\n",
    "    text = soup.get_text()\n",
    "\n",
    "    # break into lines and remove leading and trailing space on each\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    # break multi-headlines into a line each\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    # drop blank lines\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def extract_pdf(url):\n",
    "    pdf = urlretrieve(url, \"pdf_file.pdf\")\n",
    "    reader = PdfReader(\"pdf_file.pdf\")\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM = \"gemini-1.5-flash\"\n",
    "model = genai.GenerativeModel(LLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Gemini to summarize the papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:20<00:00,  6.73s/it]\n"
     ]
    }
   ],
   "source": [
    "for paper in tqdm(papers):\n",
    "    try:\n",
    "        paper[\"summary\"] = model.generate_content(\"Summarize this research article into one paragraph without formatting highlighting its strengths and weaknesses. \" + extract_pdf(paper[\"url\"])).text\n",
    "    except:\n",
    "        print(\"Generation failed\")\n",
    "        paper[\"summary\"] = \"Paper not available\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print the results to a html file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = f\"<html> <head> <h1>Daily Dose of AI Research</h1> <h4>{date.today()}</h4> <p><i>Summaries generated with: {LLM}</i>\"\n",
    "with open(\"papers.html\", \"w\") as f:\n",
    "    f.write(page)\n",
    "for paper in papers:\n",
    "    page = f'<h2><a href=\"{paper[\"url\"]}\">{paper[\"title\"]}</a></h2> <p>{paper[\"summary\"]}</p>'\n",
    "    with open(\"papers.html\", \"a\") as f:\n",
    "        f.write(page)\n",
    "end = \"</head>  </html>\"\n",
    "with open(\"papers.html\", \"a\") as f:\n",
    "    f.write(end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print the results to this notebook as markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**[AndroidLab: Training and Systematic Benchmarking of Android Autonomous Agents](https://arxiv.org/pdf/2410.24024)**<br>This research article introduces ANDROID LAB, a systematic framework for training and evaluating Android autonomous agents. The framework includes a standard operational environment with different modalities (XML and SoM) and action spaces, along with a reproducible benchmark of 138 tasks across nine apps. The researchers also developed the Android Instruct dataset, containing 10.5k traces and 94.3k steps, to fine-tune open-source models. The results show that fine-tuning improves the performance of open-source models, achieving success rates and efficiency levels comparable to closed-source models. \n",
       "\n",
       "Strengths of the research include the development of a comprehensive and reproducible benchmark, the use of both open-source and closed-source models, and the creation of a large-scale dataset for fine-tuning. However, the research is limited by the focus on a single mobile operating system (Android) and the reliance on pre-defined tasks, which may not fully capture the complexity of real-world mobile agent scenarios. Additionally, the use of the ReAct and SeeAct frameworks, while showing some promise, did not consistently enhance performance.  \n",
       "<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**[WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum Reinforcement Learning](https://arxiv.org/pdf/2411.02337)**<br>This research paper introduces WEBRL, a self-evolving online curriculum reinforcement learning framework for training large language models (LLMs) to act as web agents. WEBRL tackles three major challenges in this domain: limited training data, sparse feedback signals, and policy distribution drift in online learning. WEBRL utilizes a self-evolving curriculum that generates new tasks based on unsuccessful attempts, a robust outcome-supervised reward model (ORM) for evaluating task success, and adaptive reinforcement learning strategies to prevent catastrophic forgetting. WEBRL achieves state-of-the-art performance on the WebArena-Lite benchmark, surpassing even proprietary LLMs like GPT-4-Turbo. However, it relies heavily on the availability of an initial set of tasks and may struggle with complex or long-horizon tasks. The reliance on manually curated initial tasks is a significant weakness, making its scalability to new domains challenging.  \n",
       "<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**[DynaMath: A Dynamic Visual Benchmark for Evaluating Mathematical Reasoning Robustness of Vision Language Models](https://arxiv.org/pdf/2411.00836)**<br>This research article introduces DYNA MATH, a novel dynamic visual math benchmark designed to assess the robustness of mathematical reasoning in vision-language models (VLMs). DYNA MATH  features 501 seed questions, each represented as a Python program capable of generating diverse question variations, such as numerical value changes, geometric transformations, and function type modifications. The benchmark's strength lies in its ability to dynamically generate a large number of concrete questions, allowing for a more comprehensive evaluation of VLM performance under varying conditions. However, DYNA MATH's reliance on program-based generation might limit the inclusion of extremely complex questions, a weakness that could be addressed in future iterations. The authors evaluate various state-of-the-art VLMs on DYNA MATH, finding a significant gap between their average-case and worst-case accuracies, highlighting the models' lack of robustness to simple question variations. This research underscores the need for further research to develop VLMs with more reliable reasoning capabilities, particularly in the face of dynamic visual and textual contexts. \n",
       "<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**[Training-free Regional Prompting for Diffusion Transformers](https://arxiv.org/pdf/2411.02395)**<br>This research article proposes a training-free regional prompting method for diffusion transformers, specifically targeting the FLUX.1 architecture. The method utilizes attention manipulation to enable fine-grained compositional text-to-image generation without requiring model retraining. It leverages regional prompt-mask pairs, which can be user-defined or generated by large language models, to guide the model in generating images with specific spatial layouts and attributes. The research highlights the method's strengths in handling complex, multi-regional prompts, achieving swift and responsive image generation, and being compatible with other plug-and-play modules like LoRAs and ControlNet. However, the article acknowledges the limitation of tuning factors for optimal visual cohesion when the number of regional masks increases. The article also presents an ablation analysis of key factors and a comparison with standard FLUX.1-dev and RPG-based regional control, demonstrating the proposed method's efficiency in terms of inference speed and GPU memory consumption. Overall, the article introduces a promising approach to enhance the compositional generation capabilities of diffusion transformers, albeit with the need for further research to address the tuning complexities associated with a high number of regional masks. \n",
       "<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**[Sparsing Law: Towards Large Language Models with Greater Activation Sparsity](https://arxiv.org/pdf/2411.02335)**<br>This research paper investigates activation sparsity in large language models (LLMs), a property where a significant portion of neurons contribute weakly to the output. The authors propose a new metric, PPL-p% sparsity, which is more precise, versatile, and performance-aware than existing methods. Through extensive experiments, they discover several scaling properties of activation sparsity related to the amount of training data, activation function, width-depth ratio, and parameter scale. Strengths include the comprehensive analysis and novel metric. However, the study is limited by its focus on relatively small LLMs and the absence of computational costs in some analyses. Additionally, the sparsity metric's sensitivity to data distributions is a potential limitation. Despite these limitations, the paper provides valuable insights for designing and training sparser LLMs, enabling more efficient and interpretable models. \n",
       "<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**[How Far is Video Generation from World Model: A Physical Law Perspective](https://arxiv.org/pdf/2411.02385)**<br>This research paper investigates the ability of video generation models to learn fundamental physical laws from visual data, drawing inspiration from OpenAI's Sora. The authors evaluate generalization performance in three scenarios: in-distribution, out-of-distribution (OOD), and combinatorial generalization, using a 2D simulation testbed for object movement and collisions. While scaling models and datasets improves in-distribution and combinatorial generalization performance, the models fail to generalize well in OOD scenarios. Further analysis reveals that the models primarily exhibit \"case-based\" generalization behavior, mimicking the closest training example, rather than abstracting general physical rules. Moreover, the models prioritize certain attributes over others during case matching, with color being the most dominant factor, followed by size, velocity, and lastly, shape. This highlights that scaling alone is not sufficient for video generation models to discover fundamental physical laws, emphasizing the need for more nuanced approaches beyond simply increasing data and model size. \n",
       "<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**[LIBMoE: A Library for comprehensive benchmarking Mixture of Experts in Large Language Models](https://arxiv.org/pdf/2411.00918)**<br>This research paper introduces LibMoE, a comprehensive and modular toolkit designed to facilitate research on Mixture-of-Experts (MoE) algorithms for large language models (LLMs). LibMoE addresses the limitations of existing MoE toolkits by offering a standard benchmark for evaluating five state-of-the-art algorithms and supporting efficient training with a modular design that allows for easy customization and scalability. The paper benchmarks these algorithms on three different LLMs and 11 datasets in a zero-shot setting, demonstrating the strengths of LibMoE in streamlining research and promoting accessible large-scale studies. However, the paper also highlights weaknesses in the current understanding of MoE algorithms, particularly regarding early stopping mechanisms and the impact of architectural choices on expert selection behavior. Despite the lack of a clear winner among the evaluated algorithms, the study reveals promising research directions for future studies through its comprehensive analysis of expert selection dynamics and performance convergence.\n",
       "<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**[GenXD: Generating Any 3D and 4D Scenes](https://arxiv.org/pdf/2411.02319)**<br>This research article presents GenXD, a unified framework for generating high-quality 3D and 4D scenes from any number of condition images.  The authors tackle the challenge of 4D generation by creating a new dataset, CamVid-30K,  which uses a data curation pipeline to extract camera poses and object motion strength from videos. GenXD incorporates multiview-temporal modules to disentangle camera and object movements, allowing for seamless learning from both 3D and 4D data. The model also employs masked latent conditions to support flexible conditioning views.  While GenXD achieves promising results, the limited diversity of real-world datasets and the difficulty in capturing large camera movements with significant object motion in videos remain limitations. \n",
       "<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**[Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent](https://arxiv.org/pdf/2411.02265)**<br>This research paper introduces Hunyuan-Large, a large open-source Mixture of Experts (MoE) model with 389 billion parameters and 52 billion activated parameters. It surpasses similar-sized models in various benchmarks including language understanding, generation, reasoning, coding, long-context, and aggregated tasks. Its strengths lie in the use of large-scale synthetic data, a mixed expert routing strategy, KV cache compression, and expert-specific learning rate strategies. However, it lacks detailed information regarding the training process, including specific hyperparameters and evaluation protocols used in the benchmark comparisons.  Furthermore, the paper does not comprehensively discuss the ethical implications and potential risks of such a large language model. \n",
       "<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**[Adaptive Caching for Faster Video Generation with Diffusion Transformers](https://arxiv.org/pdf/2411.02397)**<br>This research paper introduces Adaptive Caching (AdaCache), a training-free method to accelerate video generation using Diffusion Transformers (DiTs). Recognizing that not all videos require the same level of computational effort, AdaCache caches computations within transformer blocks, selectively reusing them based on a distance metric that measures the rate-of-change in representations. This adaptive caching schedule is further enhanced by Motion Regularization (MoReg), which allocates more computations for videos with higher motion content. AdaCache shows significant speedups (up to 4.7x) without sacrificing generation quality, outperforming other training-free acceleration methods. However, AdaCache relies on hyperparameter tuning and its performance may be limited by the quality of the motion estimation, potentially leading to inconsistent generations.  While promising, the method still requires further research to fully address these limitations. \n",
       "<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**[Decoding Dark Matter: Specialized Sparse Autoencoders for Interpreting Rare Concepts in Foundation Models](https://arxiv.org/pdf/2411.00743)**<br>This research paper introduces Specialized Sparse Autoencoders (SSAEs) as a new method for interpreting rare concepts within foundation models (FMs). SSAEs, unlike general-purpose Sparse Autoencoders (SAEs), focus on specific subdomains, enabling them to learn features representing infrequent but crucial concepts. The paper presents a practical recipe for training SSAEs, including using dense retrieval for data selection and Tilted Empirical Risk Minimization (TERM) for training objectives to improve concept recall. Evaluation on standard metrics like downstream perplexity and L0 sparsity show that SSAEs effectively capture subdomain tail concepts, surpassing the capabilities of general-purpose SAEs. A case study on the Bias in Bios dataset demonstrates the practical utility of SSAEs, leading to a 12.5% increase in worst-group classification accuracy when removing spurious gender information. While SSAEs offer a powerful new tool for interpreting FMs in subdomains, they still rely on the quality of seed data and can be computationally expensive when trained with TERM. Further research is needed to address these limitations and explore the ethical implications of manipulating rare concepts within models. \n",
       "<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**[DynaSaur: Large Language Agents Beyond Predefined Actions](https://arxiv.org/pdf/2411.01747)**<br>This research article introduces DynaSaur, an LLM agent framework that dynamically creates and composes actions in real-time using Python code. This approach addresses the limitations of existing LLM agent systems that rely on predefined action sets, which restricts their flexibility and requires substantial human effort. DynaSaur enables agents to learn and adapt to new scenarios by generating new actions when necessary, improving their performance on complex tasks. The framework is evaluated on the GAIA benchmark and shows significant performance gains over existing methods, even reaching the top of the public leaderboard. However, limitations include the tendency to generate overly specific actions and the high cost of evaluation, suggesting the need for a task curriculum and further research into cost-effective evaluation methods. \n",
       "<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for paper in papers:\n",
    "    printmd(\"**[{}]({})**<br>{}<br><br>\".format(paper[\"title\"],\n",
    "                                                paper[\"url\"],\n",
    "                                                paper[\"summary\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
