{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-Context Learning\n",
    "\n",
    "\n",
    "In-context learning is a generalisation of few-shot learning where the LLM is provided a context as part of the prompt and asked to respond by utilising the information in the context.\n",
    "\n",
    "* Example: *\"Summarize this research article into one paragraph highlighting its strengths and weaknesses: [insert article text]”*\n",
    "* Example: *\"Extract all the quotes from this text and organize them in alphabetical order: [insert text]”*\n",
    "\n",
    "A very popular technique that you will learn in week 5 called Retrieval-Augmented Generation (RAG) is a form of in-context learning, where:\n",
    "* a search engine is used to retrieve some relevant information\n",
    "* that information is then provided to the LLM as context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we download some recent research papers from arXiv papers, extract the text from the PDF files and ask Gemini to summarize the articles as well as provide the main strengths and weaknesses of the papers. Finally we print the summaries to a local html file and as markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rayenebech/hel/LLM-course-2024/mvenv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import google.generativeai as genai\n",
    "from urllib.request import urlopen, urlretrieve\n",
    "from IPython.display import Markdown, display\n",
    "from pypdf import PdfReader\n",
    "from datetime import date\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select those papers that have been featured in Hugging Face papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://huggingface.co/papers\"\n",
    "page = requests.get(BASE_URL)\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "h3s = soup.find_all(\"h3\")\n",
    "\n",
    "papers = []\n",
    "\n",
    "for h3 in h3s:\n",
    "    a = h3.find(\"a\")\n",
    "    title = a.text\n",
    "    link = a[\"href\"].replace('/papers', '')\n",
    "\n",
    "    papers.append({\"title\": title, \"url\": f\"https://arxiv.org/pdf{link}\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to extract text from PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_paper(url):\n",
    "    html = urlopen(url).read()\n",
    "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "\n",
    "    # kill all script and style elements\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()    # rip it out\n",
    "\n",
    "    # get text\n",
    "    text = soup.get_text()\n",
    "\n",
    "    # break into lines and remove leading and trailing space on each\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    # break multi-headlines into a line each\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    # drop blank lines\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def extract_pdf(url):\n",
    "    pdf = urlretrieve(url, \"pdf_file.pdf\")\n",
    "    reader = PdfReader(\"pdf_file.pdf\")\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM = \"gemini-1.5-flash\"\n",
    "model = genai.GenerativeModel(LLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Gemini to summarize the papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:32<00:00,  8.10s/it]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Summarize this research article into a table highlighting its strengths and weaknesses in two different columns. \"\n",
    "for paper in tqdm(papers):\n",
    "    try:\n",
    "        paper[\"summary\"] = model.generate_content(prompt + extract_pdf(paper[\"url\"])).text\n",
    "    except:\n",
    "        print(\"Generation failed\")\n",
    "        paper[\"summary\"] = \"Paper not available\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print the results to a html file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_markdown_to_html_table(markdown_table):\n",
    "    lines = markdown_table.strip().split(\"\\n\")\n",
    "    headers = [\"Strengths\", \"Weaknesses\"]  # Fix the header line\n",
    "    rows = lines[3:]\n",
    "\n",
    "    html_table = \"<table border='1'>\\n<thead>\\n<tr>\"\n",
    "    html_table += \"\".join(f\"<th>{header.strip()}</th>\" for header in headers)\n",
    "    html_table += \"</tr>\\n</thead>\\n<tbody>\\n\"\n",
    "\n",
    "    for row in rows:\n",
    "        if set(row.strip()) == {'|', '-'}:\n",
    "            continue\n",
    "        cells = row.split(\"|\")[1:-1]\n",
    "        html_table += \"<tr>\" + \"\".join(f\"<td>{cell.strip().replace('**', '<b>').replace('**', '</b>')}</td>\" for cell in cells) + \"</tr>\\n\"\n",
    "\n",
    "    html_table += \"</tbody>\\n</table>\"\n",
    "    return html_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = f\"<html> <head> <h1>Daily Dose of AI Research</h1> <h4>{date.today()}</h4> <p><i>Summaries generated with: {LLM}</i>\"\n",
    "with open(\"papers.html\", \"w\") as f:\n",
    "    f.write(page)\n",
    "for paper in papers:\n",
    "    html_table = convert_markdown_to_html_table(paper[\"summary\"])\n",
    "    page = f'<h2><a href=\"{paper[\"url\"]}\">{paper[\"title\"]}</a></h2> <p>{html_table}</p>'\n",
    "    with open(\"papers.html\", \"a\") as f:\n",
    "        f.write(page)\n",
    "end = \"</head>  </html>\"\n",
    "with open(\"papers.html\", \"a\") as f:\n",
    "    f.write(end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print the results to this notebook as markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**[YuLan-Mini: An Open Data-efficient Language Model](https://arxiv.org/pdf/2412.17743)**<br>## YuLan-Mini: Strengths and Weaknesses\n",
       "\n",
       "| Strengths                                                                                                                                                                    | Weaknesses                                                                                                                                                                  |\n",
       "|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
       "| Achieves top-tier performance among models of similar parameter scale (2.42B).                                                                                                | Limited context window (up to 28K tokens) due to resource constraints. Performance on long-context benchmarks is not on par with state-of-the-art models.                  |\n",
       "| Data-efficient: Achieves comparable performance to industry-leading models requiring significantly more data (trained on 1.08T tokens).                                                |  Reproducing baseline model results is challenging due to lack of detailed information in their original papers. This makes the performance comparison less precise.           |\n",
       "| Open-source: Full details of data composition and training procedures are publicly available, facilitating reproducibility.                                                                 | While data-efficient, still requires significant computational resources (56 A800 GPUs initially, reduced to 48 later). Not entirely accessible to all researchers.              |\n",
       "| Elaborate data pipeline combines data cleaning, scheduling strategies, and synthetic data generation for enhanced model capabilities, particularly in math and coding.                 |  Only evaluated on a selected set of benchmarks.  A more exhaustive evaluation across diverse tasks would provide a more complete picture of the model's strengths and weaknesses. |\n",
       "| Robust optimization method effectively mitigates training instability using techniques like µP initialization, WeSaR re-parameterization, and various numerical stability improvements.| The training stability mitigation methods are extensive, but their individual contributions are not completely isolated in the ablation study, limiting precise understanding of their impact. |\n",
       "| Effective annealing approach incorporates targeted data selection and long context training to further improve performance.                                                          |   No direct comparison to truly comparable open-source models with fully disclosed training details and evaluation methodology.                                               |\n",
       "| Uses readily available open-source and synthetic data.                                                                                                                            |  Requires expertise in setting up and managing a large-scale GPU cluster.                                                                                                    |\n",
       "\n",
       "\n",
       "<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**[A Silver Bullet or a Compromise for Full Attention? A Comprehensive Study of Gist Token-based Context Compression](https://arxiv.org/pdf/2412.17483)**<br>## Gist Token-based Context Compression: Strengths and Weaknesses\n",
       "\n",
       "| Strengths | Weaknesses |\n",
       "|---|---|\n",
       "| * **Near-lossless performance on several tasks:** Achieves comparable results to full attention models on tasks like retrieval-augmented generation (RAG), long-document QA, and summarization, especially at lower compression ratios. | * **Significant performance gaps on certain tasks:**  Struggles with tasks requiring precise recall (e.g., reranking, synthetic recall) and complex multi-hop reasoning. Performance degrades significantly with increasing compression ratios. |\n",
       "| * **Efficient context compression:** Significantly reduces KV cache size and computational cost, making long-context processing more feasible for resource-constrained environments. | * **Compression bottleneck:** Gist tokens fail to fully capture original token information, leading to information loss and inaccurate reconstruction. This bottleneck is identified through probing experiments. |\n",
       "| * **Unified framework for categorization:**  Provides a structured framework for understanding existing gist-based architectures based on memory location and gist granularity. | * **Three identified failure patterns:**  \"Lost by the boundary\" (degradation near segment starts), \"Lost if surprise\" (unexpected information ignored), and \"Lost along the way\" (errors during multi-step generation). |\n",
       "| * **Proposed effective mitigation strategies:** Introduces fine-grained autoencoding and segment-wise token importance estimation to improve gist token representations and optimize learning. These strategies demonstrably improve performance, particularly on challenging tasks. | * **Limited model scale and context length in experiments:**  Experiments were constrained by computational resources, limiting the exploration of larger models and longer contexts.  The findings may not generalize perfectly to much larger models. |\n",
       "| * **Comprehensive experimental evaluation:**  Extensive experiments across various language modeling and downstream tasks provide a thorough assessment of the method's effectiveness and limitations. | * **Limited scope of compression methods:** The study focuses solely on gist token-based methods, excluding other context compression techniques.  A more comprehensive comparison across diverse methods would strengthen the conclusions. |\n",
       "| * **Addresses ethical considerations:** Uses established, well-curated datasets to minimize bias and harmful content in model training. |  |\n",
       "\n",
       "<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**[Molar: Multimodal LLMs with Collaborative Filtering Alignment for Enhanced Sequential Recommendation](https://arxiv.org/pdf/2412.18176)**<br>## Molar: Multimodal LLMs with Collaborative Filtering Alignment - Strengths and Weaknesses\n",
       "\n",
       "| Strengths                                                                                                                                                                                             | Weaknesses                                                                                                                                                                                                      |\n",
       "|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
       "| **Superior Performance:** Consistently outperforms traditional SR models and state-of-the-art LLM-based methods across multiple datasets in terms of NDCG@K and Recall@K.                                   | **Computational Cost:** Multi-task fine-tuning is time-intensive, hindering real-time applications.  The reliance on large MLLMs increases computational demands.                                                              |\n",
       "| **Effective Multimodal Integration:**  Leverages a Multimodal Large Language Model (MLLM) to effectively integrate textual and non-textual (image) data, leading to improved item representations and recommendation accuracy. | **Dependence on MLLM Quality:** Performance heavily relies on the underlying capabilities of the chosen MLLM. Suboptimal base models can lead to degraded recommendation performance.                                                     |\n",
       "| **Enhanced Collaborative Filtering:** Incorporates collaborative filtering signals through a post-alignment mechanism, effectively combining content-based and ID-based user embeddings to improve personalization. | **Limited Scalability (in the paper):**  Due to computational constraints, the authors couldn't train larger LLMs, limiting the potential performance gains.                                                                  |\n",
       "| **Robustness:** Demonstrates consistent performance improvements across diverse datasets and scenarios.                                                                                                           | **Data Dependency:** The effectiveness of the multimodal fine-tuning depends on the quality and quantity of the available multimodal data.                                                                               |\n",
       "| **Efficient Framework:** Decoupled framework (MIRM and DUEG) improves computational efficiency compared to approaches that process long user history sequences directly within the LLM.                                  | **Interpretability:** While the model performs well, understanding precisely *why* it makes specific recommendations remains challenging, a common limitation of complex deep learning models.                               |\n",
       "| **Comprehensive Evaluation:** The paper includes thorough experimentation, ablation studies, and analysis to understand the impact of different components and data modalities.                                         |  **Fine-tuning complexity:** The three-objective fine-tuning process for MIRM adds to the complexity of the training process.                                                                                          |\n",
       "\n",
       "\n",
       "<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**[MMFactory: A Universal Solution Search Engine for Vision-Language Tasks](https://arxiv.org/pdf/2412.18072)**<br>## MMFactory: Strengths and Weaknesses\n",
       "\n",
       "| Strengths                                                                                                       | Weaknesses                                                                                                                           |\n",
       "|-----------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------|\n",
       "| **Universality:** Searches across diverse vision, language, and vision-language models for solutions.             | **Computational Cost:**  Solution generation can be time-consuming, especially with a large pool of existing solutions.  High API costs if using commercial LLMs. |\n",
       "| **Multiple Solutions:** Provides a pool of programmatic solutions, allowing users to choose based on constraints. | **Complexity:** The framework is complex, requiring expertise to set up and manage.  Multi-agent system can be challenging to debug.        |\n",
       "| **User-Centric:** Considers user-defined constraints (performance, computational resources) during solution generation. | **Dependence on LLMs:** Heavily relies on the capabilities of LLMs, especially for solution and metric routing.  Performance is limited by the underlying LLMs. |\n",
       "| **Generalizable Solutions:** Solutions apply to all instances of a user-defined task, not just individual examples.      | **Limited Evaluation:**  Evaluation relies on existing benchmarks and metrics, which might not capture all aspects of task performance.   |\n",
       "| **Robust Solutions:** Uses a multi-agent LLM conversation to refine solutions, improving correctness and robustness.| **Transparency:** The decision-making process within the multi-agent system lacks complete transparency.                                         |\n",
       "| **State-of-the-art Performance:** Outperforms existing methods on benchmark datasets in several tasks.              | **Open-Source Model Reliance:** Some experiments depend on open-source models, which might not always match the performance of commercial models. |\n",
       "\n",
       "\n",
       "<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for paper in papers:\n",
    "    printmd(\"**[{}]({})**<br>{}<br><br>\".format(paper[\"title\"],\n",
    "                                                paper[\"url\"],\n",
    "                                                paper[\"summary\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
